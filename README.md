# Assessing Face Attractivity With Convolutional Networks



## Folder structure

## images/

This folder contains the input data for all the training, validation and the test splits. Therefore,
this folder has three subfolders:

- **training/**
- **validation/**
- **test/**

In each of those folders, input images are located, which are named with their regression
labels. For example, for the file name "1_AF5.jpg", the regression label is the 1 at the beginning.


### models/

Contains the model implementation script. There is only one model script in this folder. In this scirpt,
forward function for the model and the loss functions are implemented.


## outs/

This is a temporary folder for the checkpoints generated by the model summary functions. These functions
record the model weights and the layer outputs at each iteration. These checkpoints are loaded
by the tensorboard.


### config.py
This script contains the model and data preprocessing parameters. Before running the code, 
parameters will be modified here. Available parameters are as listed below:

- **Model type**: Type of the model to run. In the report, two models are introduced, the
              LargeNet (selected with "large") and the CharmNet (selected with "compact").
              
- **Batch Size**: Size of the batches for the mini-batches.
- **Epoch Numbers**: Number of training epochs.
- **Weight Initialization**: Weight initialization method. Can be either "gaussian" or "xavier".
- **Learning Rate**: Learning rate of the parameters.
- **Batch Normalization**: Flag for toggling the batch normalization layers between the convolutional
                           and the dense layers.
- **Loss Type**: Type of the loss function to be used in the training. Can be "l1" or "l2" for the
                 L1 and L2 losses.
- **L2 Loss**: Flag toggling the L2 weight regularization.
- **alpha**: Loss weight for the regularization loss.
- **Dropout**: Toggles the dropout layers between the dense layers.
- **Keep Rate**: Keep probability for the neurons in the dense layers.                           
         



### data_processor.py

This script contains the implementation of the data processor class. This class reads the images
from the default data folder path which is "./images/". For each of the training, validation or the
test split, all data is read and their labels are stored separately. 

During training, random mini-batches are generated from the whole list of images. These batches are
given to the model for each call. For the validation and the test sets, whole data is returned 
without generating mini-batches.



### helper.py

In this script, we have imported a wrapper function to be used in the model implementation. This
wrapper is the lazy_property written by the

https://danijar.com/structuring-your-tensorflow-models/


### main.py

This is the main script of the project. It does not accept any arguments and takes the run
configurations from the "config.py". Here, the parameter and layer logging functions are initialized.
Moreover, the model and the data are prepared.

Training is done in a main for loop which iterates over the whole data for the specified number of
epochs. 

## How to run?

We are using the Tensorflow 2.2 with the GPU support. In addition, the scikit-image is needed. After
the requirements are installed with the following command,


```bash
pip3 install -r requirements.txt
```

the main script can be run with the following command,

```bash
python3 main.py
```


When the training starts, print statements will inform the user about the training procedure. In addition
to the verbosity provided by the print statements, it is possible to inspect the model weights and
the layer histograms via tensorboard with the following command,

```bash
tensorboard --logdir ./
```
